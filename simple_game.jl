# following code is from Algorithms for Decision Making

struct SimpleGame
    Î³   # discount factor
    â„   # agents
    ğ’œ   # joint action space
    R   # joint reward function
end

struct SimpleGamePolicy
    p   # dictionary mapping actions to probabilities
    function SimpleGamePolicy(p::Base.Generator)
        return SimpleGamePolicy(Dict(p))
    end
    
    function SimpleGamePolicy(p::Dict)
        vs=collect(values(p))
        vs./=sum(vs)
        return new(Dict(k => v for (k,v) in zip(keys(p),vs)))
    end
    SimpleGamePolicy(ai)=new(Dict(ai=>1.0))
end

(Ï€i::SimpleGamePolicy)(ai)=get(Ï€i.p,ai,0.0)

function(Ï€i::SimpleGamePolicy)()
    D=SetCategorical(collect(keys(Ï€i.p)),collect(values(Ï€i.p)))
    return rand(D)
end

struct NashEquilibrium end

function tensorform(ğ’«::SimpleGame)
    â„,ğ’œ,R=ğ’«.â„,ğ’«.ğ’œ,ğ’«.R
    â„â€²=eachindex(â„)
    ğ’œâ€²=[eachindex(ğ’œ[i]) for i in â„]
    Râ€²=[R(a) for a in joint(ğ’œ)]
    return â„â€²,ğ’œâ€²,Râ€²
end

function solve(M::NashEquilibrium,ğ’«::SimpleGame)
    â„,ğ’œ,R=tensorform(ğ’«)
    model=Model(Ipopt.Optimizer)
    @variable(model,U[â„])
    @variable(model,Ï€[i=â„,ğ’œ[i]]â‰¥0)
    @NLobjective(model,Min,sum(U[i]-sum(prod(Ï€[j,a[j]] for j in â„)*R[y][i] for (y,a) in enumerate(joint(ğ’œ))) for i in â„))
    @NLconstraint(model, [i=â„,ai=ğ’œ[i]],U[i]â‰¥sum(prod(j == i ? (a[j] == ai ? 1.0 : 0.0) : Ï€[j,a[j]] for j in â„) * R[y][i] for (y,a) in enumerate(joint(ğ’œ))))
    @constraint(model, [i=â„],sum(Ï€[i,ai] for ai in ğ’œ[i]) == 1)
    optimize!(model)
    Ï€iâ€²(i) = SimpleGamePolicy(ğ’«.ğ’œ[i][ai]=>value(Ï€[i,ai]) for ai in ğ’œ[i])
    return [Ï€iâ€²(i) for i in â„]
end