
using JuMP, GLPK
###     Policy Evaluation   ###
#   valuating Conditional Plans

include("support_code.jl")
include("multi_caregiver.jl")

struct ConditionalPlan
    a   # action to take at root
    subplans    # dictionary mapping observations to subplans
end

ConditionalPlan(a) = ConditionalPlan(a, Dict())
(Ï€::ConditionalPlan)() = Ï€.a
(Ï€::ConditionalPlan)(o) = Ï€.subplans[o]

function ConditionalPlan(ğ’«::POMG,a,plans)
    subplans=Dict(o => Ï€ for (o,Ï€) in zip(ğ’«.ğ’ª,plans))
    return ConditionalPlan(a,subplans)
end


struct AlphaVectorPolicy
    ğ’«   # POMDP problem
    Î“   # alpha vectors
    a   # actions associated with alpha vectors
end

function utility(Ï€::AlphaVectorPolicy,b)
    return maximum(Î±â‹…b for Î± in Ï€.Î“)
end

function (Ï€::AlphaVectorPolicy)(b)
    i=argmax([Î±â‹…bforÎ±inÏ€.Î“])
    return Ï€.a[i]
end

function find_maximal_belief(Î±,Î“)
    m=length(Î±)
    if isempty(Î“)
        return fill(1/m,m)   # arbitrary belief
    end
    model=Model(GLPK.Optimizer)
    @variable(model,Î´)
    @variable(model,b[i=1:m]â‰¥0)
    @constraint(model,sum(b)==1.0)
    for a in Î“ 
        @constraint(model, (Î±-a)â‹…bâ‰¥Î´)
    end
    @objective(model,Max,Î´)
    optimize!(model)
    return value(Î´)>0 ? value.(b) : nothing
end

function find_dominating(Î“)
    n=length(Î“)
    candidates,dominating=trues(n),falses(n)
    # loop still there are no any candidates
    while any(candidates)
        # get the first found candidate
        i=findfirst(candidates)
        # find maximum belief
        b=find_maximal_belief(Î“[i],Î“[dominating])
        if b===nothing
            # cannt find a maximal belief associated with candidate i
            candidates[i]=false
        else
            # Otherwise, i is a dominating plan
            k=argmax([candidates[j] ? bâ‹…Î“[j] : -Inf for j in 1:n])
            candidates[k],dominating[k]=false,true
        end
    end
    return dominating
end


function prune(plans,Î“)
    # list of dominating plan
    d=find_dominating(Î“)
    return(plans[d],Î“[d])
end

# joint_action(x) = vec(collect(Iterators.product(x...)))
joint_action(x) = [(e, e) for e in x[1]]
joint_observation(x) = [(e, e) for e in x[1]]

function value_iteration(ğ’«::POMG,k_max)
    ğ’®, ğ’œ, R = ğ’«.ğ’®, joint_action(ğ’«.ğ’œ), ğ’«.R

    # get joint action space 
    plans = [ConditionalPlan(a) for a in ğ’œ]

    # calculate 1-step utility
    Î“ = [[sum(R(s,a)) for s in ğ’®] for a in ğ’œ]

    # remove dominated plans 
    plans,Î“= prune(plans,Î“) 

    # iteration step 
    for k in 2:k_max
        plans, Î“ = expand(plans, Î“, ğ’«)
        plans, Î“ = prune(plans, Î“)
    end
    return (plans,Î“)
end


function combine_lookahead(ğ’«::POMG,s,a,Î“o)
    ğ’®,ğ’ª,T,O,R,Î³=ğ’«.ğ’®,joint_observation(ğ’«.ğ’ª),ğ’«.T,ğ’«.O,ğ’«.R,ğ’«.Î³
    Uâ€²(sâ€²,i)=sum(O(a,sâ€²,o)*Î±[i] for (o,Î±) in zip(ğ’ª,Î“o))
    return sum(R(s,a))+Î³*sum(T(s,a,sâ€²)*Uâ€²(sâ€²,i) for (i,sâ€²) in enumerate(ğ’®))
end

function combine_alphavector(ğ’«::POMG,a,Î“o)
    return [combine_lookahead(ğ’«,s,a,Î“o) for s in ğ’«.ğ’®]
end


function expand(plans,Î“,ğ’«)
    ğ’®,ğ’œ,ğ’ª,T,O,R=ğ’«.ğ’®,joint_action(ğ’«.ğ’œ),joint_observation(ğ’«.ğ’ª),ğ’«.T,ğ’«.O,ğ’«.R
    plansâ€²,Î“â€²=[], []
    for a in ğ’œ
        # iterate over all possible mappings from observations to plans
        for inds in Iterators.product([eachindex(plans) for o in ğ’ª]...)
            Ï€o=plans[[inds...]]
            Î“o=Î“[[inds...]]
            Ï€=ConditionalPlan(ğ’«,a,Ï€o)
            Î±=combine_alphavector(ğ’«,a,Î“o)
            push!(plansâ€²,Ï€)
            push!(Î“â€²,Î±)
        end
    end
    return(plansâ€²,Î“â€²)
end

struct ValueIteration
    k_max   # maximum number of iterations
end

struct LookaheadAlphaVectorPolicy
    ğ’«   # POMDP problem
    Î“   # alpha vectors
end

function solve(M::ValueIteration,ğ’«::POMG)
    plans, Î“ = value_iteration(ğ’«, M.k_max)
    return plans, Î“
    return LookaheadAlphaVectorPolicy(ğ’«,Î“)
end

#=
M = ValueIteration(3)
cryingBaby = BabyPOMG()
P = POMG(cryingBaby)

plan, x = solve(M, P)
print(plan)
=#
